{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b5dcdc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\iq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\iq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\iq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\iq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from html import unescape\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import sys\n",
    "sys.path.append('./helpers')\n",
    "from preprocessing import preprocess_text\n",
    "\n",
    "\n",
    "import pickle \n",
    "import os\n",
    "import scipy\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "\n",
    "import psycopg2\n",
    "import gensim\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('gutenberg')\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from gensim.models import Word2Vec as w2v\n",
    "\n",
    "def get_embedding(x, out=False):\n",
    "    if x in model1.wv.key_to_index:\n",
    "        if out:\n",
    "            return model1.syn1neg[model1.wv.key_to_index[x]]\n",
    "        else:\n",
    "            return model1.wv[x]\n",
    "    else:\n",
    "        return np.zeros(100)\n",
    "\n",
    "\n",
    "\n",
    "# Connect to the PostgreSQL database\n",
    "conn = psycopg2.connect(\n",
    "    host=\"localhost\",\n",
    "    database=\"SearchEngine\",\n",
    "    user=\"postgres\",\n",
    "    password=\"2580\"\n",
    ")\n",
    "cursor = conn.cursor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2e8946c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\iq\\\\OneDrive\\\\books\\\\ML_DS_AI\\\\B_InformationRetrievall736P\\\\search_engine_iqFork'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32476d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.bbc.com/sitemaps/https-index-com-news.xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34203b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(url)\n",
    "if res.status_code != 200:\n",
    "    print('ohho')\n",
    "\n",
    "soup = BeautifulSoup(res.text, 'xml')\n",
    "\n",
    "sitemap_tags = soup.find_all('sitemap')\n",
    "\n",
    "sitemaps = []\n",
    "for sitemap in sitemap_tags:\n",
    "    loc = sitemap.find('loc')\n",
    "    sitemaps.append(loc.text)\n",
    "\n",
    "urls = []\n",
    "regex_1 = r\"^https://www.bbc.com/news/.*$\"\n",
    "regex_2 = r\"^https://www.bbc.com/sport/.*$\"\n",
    "for sitemap in sitemaps:\n",
    "    response = requests.get(sitemap)\n",
    "    soup = BeautifulSoup(response.text, 'xml')\n",
    "    url_tags = soup.find_all('url')\n",
    "    \n",
    "    for url in url_tags:\n",
    "        loc = url.find('loc')\n",
    "        if re.match(regex_1, loc.text) or re.match(regex_2, loc.text):\n",
    "            urls.append(loc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ffc42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = urls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26fa3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_to_text(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            return None\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        full_article = ''\n",
    "        text = []\n",
    "        heading = soup.select('h1')[0].text.strip()\n",
    "        text.append(heading)\n",
    "        \n",
    "        for p in soup.select('p'):\n",
    "            text.append(unescape(p.text))\n",
    "        full_article = ' '.join(text)\n",
    "        article_clean = preprocess_text(full_article)\n",
    "        return (url,heading, article_clean) \n",
    "    except requests.exceptions.Timeout as ee :\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c719798",
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_to_text(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            print('ohho')\n",
    "            return None\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        full_article = ''\n",
    "        text = []\n",
    "        heading = soup.select('h1')[0].text.strip()\n",
    "        text.append(heading)\n",
    "        \n",
    "        for p in soup.select('p'):\n",
    "            text.append(unescape(p.text))\n",
    "        full_article = ' '.join(text)\n",
    "        \n",
    "        full_article_part = full_article[:600]\n",
    "        article_clean = preprocess_text(full_article)\n",
    "        return (url,heading, article_clean,full_article_part )\n",
    "    \n",
    "    except requests.exceptions.Timeout as ee :\n",
    "        print('TO')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026add2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.bbc.com/news/uk-england-humber-65173206'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f90bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_to_text(url)[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7700c728",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "# Connect to the PostgreSQL database\n",
    "conn = psycopg2.connect(\n",
    "    host=\"localhost\",\n",
    "    database=\"SearchEngine\",\n",
    "    user=\"postgres\",\n",
    "    password=\"2580\"\n",
    ")\n",
    "\n",
    "# Create a cursor object\n",
    "cursor = conn.cursor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3a97a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"SELECT * FROM sites where id = 1;\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0a2be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "    cursor.execute(query)\n",
    "    results = cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4015a063",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in results:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1a4921",
   "metadata": {},
   "outputs": [],
   "source": [
    "(row[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6217effd",
   "metadata": {},
   "source": [
    "## disco "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "814ba55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query = f\"SELECT content FROM sites;\"\n",
    "cursor.execute(query)\n",
    "results = cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21878d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lines = [line[0].rstrip('\\n').lower() for line in results]\n",
    "lines = [word_tokenize(line) for line in lines]\n",
    "filtered_lines = remove_stops(text = lines, words = sw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eae1cbf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['infected blood inquiry recognise wider families daughter man died contracting hiv nhs blood products hoping new report recognise children parents victims katie walford said father david hatton died april 1998 contracting virus treated haemophilia called financial compensation wider families victims interim report issue expected released wednesday mr hatton contracted hiv hepatitis c given protein made blood plasma known factor viii thought tens thousands nhs patients infected 1970 1991 time uk factor concentrate often imported united states prisoners groups paid donate inquiry scandal ordered government october 2022 midway investigation surviving victims bereaved partners received interim compensation payments however wider families yet receive compensation inquiry due file second 